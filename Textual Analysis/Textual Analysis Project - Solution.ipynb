{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Analysis Project\n",
    "\n",
    "In this project, you will calculate disclosure tone for firms' earnings announcement 8-Ks in 2019 Q1. You will then calculate cumulative abnormal returns for the [0,+1] window surrounding the earnings announcement date and test the relation between earnings announcement tone and cumulative abnormal returns.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "#### Import Modules\n",
    "\n",
    "1. Import the following modules:\n",
    "    1. requests\n",
    "    2. pandas as pd\n",
    "    3. word_tokenize from nltk.tokenize\n",
    "    4. Counter from collections\n",
    "    5. statsmodels.api as sm \n",
    "    6. html_to_text from MyFunctions (you will need to download the **MyFunctions.py** file and save it in your current working directory)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "from MyFunctions import html_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Negative and Positive Word Lists\n",
    "\n",
    "1. Import the negative word list into a **pandas** DataFrame called **neg_words** using the **pd.read_excel** function.\n",
    "2. Import the postive word list into a **pandas** DataFrame called **pos_words** using the **pd.read_excel** function.\n",
    "3. Rename the column header in the **neg_words** and **pos_words** DataFrames to 'token'.\n",
    "3. Convert the words in the **neg_words** and **pos_words** DataFrames into lower case.\n",
    "\n",
    "The word lists included in the **LoughranMcDonald_SentimentWordLists** Excel file available at https://sraf.nd.edu/textual-analysis/resources/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Import Negative and Positive Word Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = pd.read_excel('LoughranMcDonald_SentimentWordLists_2018.xlsx', sheet_name='Negative', header=None)\n",
    "neg_words = neg_words.rename(columns={0: \"token\"})\n",
    "neg_words['token'] = neg_words['token'].str.lower()\n",
    "\n",
    "pos_words = pd.read_excel('LoughranMcDonald_SentimentWordLists_2018.xlsx', sheet_name='Positive', header=None)\n",
    "pos_words = pos_words.rename(columns={0: \"token\"})\n",
    "pos_words['token'] = pos_words['token'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Function to Obtain Tone\n",
    "\n",
    "Create a new function called **get_tone** which takes the URL of an EDGAR filing and returns the **net_tone** of the filing, where **net_tone** is calculated as follows:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "NET\\ TONE = \\frac{\\#\\ POS\\ WORDS - \\#\\ NEG\\ WORDS}{\\#\\ POS\\ WORDS + \\#\\ NEG\\ WORDS}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "See the **Disclosure Tone** module for additional instruction on how to create this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Create a Function to Obtain Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tone(url):\n",
    "    \n",
    "    # Obtain the text of the disclosure\n",
    "\n",
    "    headers = {'User-Agent': 'ORGANIZATION youremail@yourinstitution.edu'} # AUG 2021 UPDATE -- YOU HAVE TO DECLARE A HEADER TO ACCESS THE EDGAR WEBSITE\n",
    "    disclosure = requests.get(url,header=headers).text\n",
    "    text = html_to_text(disclosure)\n",
    "    \n",
    "    # Convert text to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Create tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Count tokens\n",
    "    counts = Counter(tokens)\n",
    "\n",
    "    # Create a DataFrame of our token counts\n",
    "    df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "    df = df.rename(columns={\"index\": \"token\", 0: \"count\"})\n",
    "    df = df.sort_values(by=[\"count\"],ascending=[False])\n",
    "    \n",
    "    # Merge with positive dictionary\n",
    "    data = pd.merge(df, pos_words, on='token', how='left', indicator=True)\n",
    "    data['pos'] = 0\n",
    "    data.loc[data._merge == 'both', 'pos'] = 1\n",
    "    data = data.drop(columns=['_merge'])\n",
    "    \n",
    "    # Merge with negative dictionary\n",
    "    data = pd.merge(data, neg_words, on='token', how='left', indicator=True)\n",
    "    data['neg'] = 0\n",
    "    data.loc[data._merge == 'both', 'neg'] = 1\n",
    "    data = data.drop(columns=['_merge'])\n",
    "\n",
    "    # Calculate number of positive and negative tokens\n",
    "    num_neg = data[data.neg == 1]['count'].sum()\n",
    "    num_pos = data[data.pos == 1]['count'].sum()\n",
    "    \n",
    "    # Calculate tone\n",
    "    net_tone = (num_pos - num_neg)/(num_pos + num_neg)\n",
    "    \n",
    "    return net_tone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Earnings Announcement 8-K URLs\n",
    "\n",
    "Import the '8-K URLs.txt' file into a **pandas** DataFrame called **urls** using the **pd.read_csv** function. Note: The columns are delimited with the '|' symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Import Earnings Announcement 8-K URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv('8-K URLs.txt',sep='|')\n",
    "urls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Tone\n",
    "\n",
    "1. Create a new file called 'tone.txt' in your current working directory using the **open** function. Write a new header containing the following variables delimited with the '|' symbol: **cik**, **ticker**, **date_filed**, **url**, and **net_tone**.\n",
    "2. Loop through the filings in the **urls** DataFrame using the **iterrows** function and compute the **net_tone** of each filing using the **get_tone** function. For each filing, write a new row to the 'tone.txt' file containing the following variables delimited with a '|' symbol: **cik**, **ticker**, **date_filed**, **url**, and **net_tone**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Compute Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('tone.txt','w')\n",
    "file1.write('cik|ticker|date_filed|url|net_tone\\n')\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones1 = open('tone.txt').readlines()\n",
    "dones = {}\n",
    "for done in dones1:\n",
    "    try:\n",
    "        cik = done.split('|')[0]\n",
    "        date_filed = done.split('|')[2]\n",
    "        dones[cik+'|'+date_filed]=1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "for index,row in urls.iterrows():\n",
    "    cik = str(row['cik'])\n",
    "    ticker = row['ticker']\n",
    "    date_filed = row['date_filed']\n",
    "    url = row['URL']\n",
    "    \n",
    "    try:\n",
    "        done = dones[cik+'|'+date_filed]\n",
    "    except Exception:\n",
    "        done = 0\n",
    "        \n",
    "    if done == 0:\n",
    "        try:\n",
    "            net_tone = get_tone(url)\n",
    "            file1 = open('tone.txt','a')\n",
    "            file1.write(cik+'|'+ticker+'|'+date_filed+'|'+url+'|'+str(net_tone)+'\\n')\n",
    "            file1.close()\n",
    "        except Exception:\n",
    "            print(cik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tone Data\n",
    "\n",
    "Import the 'tone.txt' file to a new **pandas** DataFrame called **data** using the **pd.read_csv** function. Use the **parse_dates** option to read in the **date_filed** variable as a **datetime** object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Import Tone Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tone.txt', sep='|', parse_dates=['date_filed'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Cumulative Abnormal Returns\n",
    "\n",
    "1. Import the 'ret.csv' file to a new **pandas** DataFrame called **ret** using the **pd.read_csv** function. Use the **parse_dates** option to read in the **Date** variable as a datetime object. The 'ret.csv' file was created by scraping Yahoo! Finance (see the Web Scraping Yahoo! Finance Tutorials) and contains the following variables:\n",
    "    1. **ticker**\n",
    "    2. **Date**\n",
    "    3. **ret** - Raw one-day return\n",
    "    4. **ewret** - Equal-weighted index return\n",
    "2. Calculate **abnret** as **ret** - **ewret**.\n",
    "3. Sort the **ret** DataFrame by **ticker** (ascending order) and **Date** (descending order) using the **sort_values** function and the **ascending** option.\n",
    "4. Use the **groupby** and **shift** functions to create a new variable called **abnret_lead1** equal to the return for each ticker on the subsequent trading date (i.e., date *t+1*). \n",
    "5. Create a new variable called **car01** equal to the cumulative abnormal return from day 0 to day +1 using the following formula:\n",
    "\n",
    "    $\n",
    "    \\begin{align}\n",
    "    CAR\\ [0,+1]\\ =\\ [\\ (1 + r_{t})\\ \\times\\ (1 + r_{t+1})\\ ] - 1\n",
    "    \\end{align}\n",
    "    $\n",
    "\n",
    "    where $r$ is the abnornal return on day t (current date) and day t+1 (subsequent trading date)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Calculate Cumulative Abnormal Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Import data\n",
    "\n",
    "ret = pd.read_csv('ret.csv', parse_dates=['Date'])\n",
    "ret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Calculate abnret\n",
    "\n",
    "ret['abnret'] = ret['ret'] - ret['ewret']\n",
    "ret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Sort data\n",
    "\n",
    "ret = ret.sort_values(by=['ticker','Date'], ascending=[True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Calculate abnret_lead1\n",
    "\n",
    "ret['abnret_lead1'] = ret.groupby('ticker')['abnret'].shift(1)\n",
    "ret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Calculate car01\n",
    "\n",
    "ret['car01'] = ((ret['abnret']+1) * (ret['abnret_lead1']+1)) - 1\n",
    "ret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the Data and Ret DataFrames\n",
    "\n",
    "1. Merge the **data** DataFrame with the **ret** DataFrame and call the resulting DataFrame **data**. Merge on **ticker** and **date_filed** in the **data** DataFrame and on **ticker** and **Date** in the **ret** DataFrame. Use an inner merge.\n",
    "2. Drop all N/A values in the **data** DataFrame using the **dropna** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Merge the Data and Ret DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, ret, left_on=['ticker','date_filed'], right_on=['ticker','Date'], how='inner')\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a Basic OLS Regression of Cumulative Abnormal Returns on Tone\n",
    "\n",
    "1. Examine the summary statistics of the **net_tone** and **car01** variables using the **describe** function. What is the median value of **net_tone**?\n",
    "2. Use the **OLS** function from the **statsmodels.api** module to run a basic OLS regression with the **car01** as the dependent variable and **net_tone** as the independent variable. Include a constant term in the regression. Print the OLS model summary output using the **summary** function.\n",
    "3. Comment on the relation between tone and the cumulative abnormal return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution - Run a Basic OLS Regression of Cumulative Abnormal Returns on Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Summary Statistics\n",
    "\n",
    "data[['net_tone','car01']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 OLS Regression\n",
    "\n",
    "X = data[\"net_tone\"]\n",
    "y = data[\"car01\"]\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "# There is a positive and statistically significant relation between net_tone and car01."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
