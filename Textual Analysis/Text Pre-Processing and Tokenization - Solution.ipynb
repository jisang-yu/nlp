{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing and Tokenization\n",
    "\n",
    "In this module, we'll learn how to create \"tokens\" based on the text of a document and how to pre-process the text to exclude things like punctuation and stop words (e.g., \"the\", \"an\", \"and\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, we'll use a small snippet from Steve Jobs' 2005 Stanford Commencement Address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I was lucky — I found what I loved to do early in life. Woz and I started Apple in my parents' garage when I was 20. We worked hard, and in 10 years Apple had grown from just the two of us in a garage into a $2 billion company with over 4,000 employees. We had just released our finest creation — the Macintosh — a year earlier, and I had just turned 30. And then I got fired. How can you get fired from a company you started? Well, as Apple grew we hired someone who I thought was very talented to run the company with me, and for the first year or so things went well. But then our visions of the future began to diverge and eventually we had a falling out. When we did, our Board of Directors sided with him. So at 30 I was out. And very publicly out. What had been the focus of my entire adult life was gone, and it was devastating. I really didn't know what to do for a few months. I felt that I had let the previous generation of entrepreneurs down — that I had dropped the baton as it was being passed to me. I met with David Packard and Bob Noyce and tried to apologize for screwing up so badly. I was a very public failure, and I even thought about running away from the valley. But something slowly began to dawn on me — I still loved what I did. The turn of events at Apple had not changed that one bit. I had been rejected, but I was still in love. And so I decided to start over. I didn't see it then, but it turned out that getting fired from Apple was the best thing that could have ever happened to me. The heaviness of being successful was replaced by the lightness of being a beginner again, less sure about everything. It freed me to enter one of the most creative periods of my life. During the next five years, I started a company named NeXT, another company named Pixar, and fell in love with an amazing woman who would become my wife. Pixar went on to create the world's first computer animated feature film, Toy Story, and is now the most successful animation studio in the world. In a remarkable turn of events, Apple bought NeXT, I returned to Apple, and the technology we developed at NeXT is at the heart of Apple's current renaissance. And Laurene and I have a wonderful family together. I'm pretty sure none of this would have happened if I hadn't been fired from Apple. It was awful tasting medicine, but I guess the patient needed it. Sometimes life hits you in the head with a brick. Don't lose faith. I'm convinced that the only thing that kept me going was that I loved what I did. You've got to find what you love. And that is as true for your work as it is for your lovers. Your work is going to fill a large part of your life, and the only way to be truly satisfied is to do what you believe is great work. And the only way to do great work is to love what you do. If you haven't found it yet, keep looking. Don't settle. As with all matters of the heart, you'll know when you find it. And, like any great relationship, it just gets better and better as the years roll on. So keep looking until you find it. Don't settle.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Tokenization is the process of turning text string into tokens (i.e., smaller chunks). \n",
    "\n",
    "Examples of tokenization include:\n",
    "\n",
    "1. Obtaining all words in a sentence\n",
    "2. Obtaining all sentences in a document\n",
    "3. Obtaining regex patterns in a document\n",
    "\n",
    "We tokenize text for several reasons. The most important is that we need a way to summarize the information in our text. By splitting the text into smaller chunks, we can create summary measures that capture the content of the text (e.g., tone).\n",
    "\n",
    "We will use the **nltk** (natural language toolkit) module to perform tokenization. \n",
    "\n",
    "The **nltk** module has several built-in tokenizers:\n",
    "\n",
    "1. **word_tokenize**: tokenize a document into words\n",
    "2. **sent_tokenize**: tokenize a document into sentences\n",
    "3. **regexp_tokenize**: tokenize a string or document based on a regular expression\n",
    "\n",
    "We can import these tokenizers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "    \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's play with these tokenizers to see what they do.\n",
    "\n",
    "#### Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex Tokens\n",
    "\n",
    "This code obtains all words that start with capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_words = regexp_tokenize(text, '[A-Z][a-zA-Z]+')\n",
    "cap_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** This function is similar to the re.findall function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_words = re.findall('[A-Z][a-zA-Z]+', text)\n",
    "cap_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Tokens\n",
    "\n",
    "We can use the **Counter** function to count the number of tokens in our text. We import the **Counter** function from the **collections** module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then obtain counts for each token using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "counts = Counter(tokens)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **most_common** function to print the tokens and their counts in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "\n",
    "We can see in the above example that the most common tokens will inevitably be words like \"the\" and \"of\", even though these words do not truly represent the theme of the underlying text.  We can perform several text preprocessing steps to make our tokens more meaningful. Here are a few common text preprocessing steps:\n",
    "\n",
    "1. Convert all text to lower case to avoid separately counting \"Event\" and \"event\", for example.\n",
    "2. Shorten words to their root stems to avoid separately counting \"computer\" and \"computers\", for example. \n",
    "3. Remove stop words (e.g., \"and\", \"the\", \"an\", etc.)\n",
    "4. Remove punctuation (e.g., \".\", \"?\", \"-\", etc.)\n",
    "\n",
    "The **nltk** module has a built-in list of stop words. We can access this list using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **nltk** module has a built-in function called **PorterStemmer** that we can use to stem words to their roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "words = ['caring', 'helpful', 'kindness', 'jumps']\n",
    "for word in words:\n",
    "    stemmed_word = porter.stem(word)\n",
    "    print(stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply these filters to our text and then check the most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain word tokens\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Convert tokens to lower case\n",
    "tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Keep tokens that are alphabetic (i.e., remove numbers and punctuation)\n",
    "tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "# Remove stop words\n",
    "tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "# Stem all tokens\n",
    "tokens = [porter.stem(t) for t in tokens]\n",
    "\n",
    "# Obtain token counts\n",
    "counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Counted Tokens Dictionary into a Pandas DataFrame\n",
    "\n",
    "To make our **counts** more easily useable, we can convert them into a pandas DataFrame using the **pd.DataFrame.from_dict** function. We use the **from_dict** function because the **Counter** function stores the **counts** in a dictionary format where the tokens are the keys and the counts are the values. We'll use the **orient='index'** option to create the DataFrame using the dictionary keys as rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "df = df.rename(columns={\"index\": \"token\", 0: \"count\"})\n",
    "df = df.sort_values(by=[\"count\"],ascending=[False])\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "1. Use the **requests.get** function to obtain the html source code of the April 30, 2020 earnings announcement 8-K for Apple available at the following link: https://www.sec.gov/Archives/edgar/data/320193/000032019320000050/a8-kexhibit991q2202032.htm\n",
    "2. Import the **html_to_text** function from the **MyFunctions** module. The **html_to_text** function takes a variable containing html source code in string format as an input, strips out all html code from this variable, and returns a string variable containing only the text of the html source code. Use the **html_to_text** function to convert the earnings announcement 8-K html data into a text format.\n",
    "3. Obtain a list of all tokens in the 8-K using the **word_tokenize** function.\n",
    "4. Perform the following text-preprocessing steps:\n",
    "    1. Convert all tokens to lower case using the **lower** function.\n",
    "    2. Remove tokens in the stop words list using the **stopwords** function.\n",
    "    3. Remove tokens that are punctuation.\n",
    "    4. Shorten all tokens to their root stems using the **PorterStemmer** function.\n",
    "5. Obtain all remaining token counts and place them in a new **pandas** DataFrame. Sort the DataFrame in descending order by count and print out the first 10 rows. What are the most common words used in Apple's 8-K?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution for # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUG 2021 UPDATE -- YOU HAVE TO DECLARE A HEADER TO ACCESS THE EDGAR WEBSITE\n",
    "\n",
    "headers = {'User-Agent': 'ORGANIZATION youremail@yourinstitution.edu'}\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/320193/000032019320000050/a8-kexhibit991q2202032.htm'\n",
    "data = requests.get(url, headers=headers).text \n",
    "print(data[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution for # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyFunctions import html_to_text\n",
    "\n",
    "text = html_to_text(data)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution for # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution for # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokens to lower case\n",
    "tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Keep tokens that are alphabetic (i.e., remove numbers and punctuation)\n",
    "tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "# Remove stop words\n",
    "tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "# Stem all tokens\n",
    "tokens = [porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution for # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(tokens)\n",
    "\n",
    "df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "df = df.rename(columns={\"index\": \"token\", 0: \"count\"})\n",
    "df = df.sort_values(by=[\"count\"],ascending=[False])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
